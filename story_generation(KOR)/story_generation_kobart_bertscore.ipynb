{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b4e8954",
   "metadata": {},
   "source": [
    "### 토크나이저 및 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e97720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\torch-py38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu') # gpu\n",
    "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2') # tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2').to(device) # model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca3fb1",
   "metadata": {},
   "source": [
    "### 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676b2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c021902",
   "metadata": {},
   "source": [
    "**Train Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f2e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "summaries = []\n",
    "\n",
    "# [ AI-HUB ] 요약문 및 레포트 생성 데이터 : 두 개의 폴더에 나눠져 있는 데이터를 리스트에 모아준다\n",
    "## 2~3sent\n",
    "for f_name in glob(os.path.join(\"D:/jupyter/data/요약문_및_레포트_생성/train\",\"literature/2~3sent/*.json\")) :\n",
    "    with open(f_name, 'r', encoding = 'utf-8') as f :\n",
    "        json_data = json.load(f)\n",
    "        passages.append(json_data['Meta(Refine)']['passage'])\n",
    "        summaries.append(json_data['Annotation']['summary1'])\n",
    "\n",
    "## 20per\n",
    "for f_name in glob(os.path.join(\"D:/jupyter/data/요약문_및_레포트_생성/train\",\"literature/20per/*.json\")) :\n",
    "    with open(f_name, 'r', encoding = 'utf-8') as f :\n",
    "        json_data = json.load(f)\n",
    "        passages.append(json_data['Meta(Refine)']['passage'])\n",
    "        summaries.append(json_data['Annotation']['summary1'])\n",
    "        \n",
    "## 한자 제거\n",
    "passages = [re.sub(\"\\([^\\(\\)]+\\)\", \"\", passage) for passage in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2195e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To Dataframe\n",
    "train_df = pd.DataFrame([ x for  x in zip(summaries, passages)], columns=[\"input_texts\",\"target_texts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39aa995",
   "metadata": {},
   "source": [
    "**Valid Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32d5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "summaries = []\n",
    "\n",
    "# [ AI-HUB ] 요약문 및 레포트 생성 데이터 : 두 개의 폴더에 나눠져 있는 데이터를 리스트에 모아준다\n",
    "## 2~3sent\n",
    "for f_name in glob(os.path.join(\"D:/jupyter/data/요약문_및_레포트_생성/valid\",\"literature/2~3sent/*.json\")) :\n",
    "    with open(f_name, 'r', encoding = 'utf-8') as f :\n",
    "        json_data = json.load(f)\n",
    "        passages.append(json_data['Meta(Refine)']['passage'])\n",
    "        summaries.append(json_data['Annotation']['summary1'])\n",
    "\n",
    "## 20per\n",
    "for f_name in glob(os.path.join(\"D:/jupyter/data/요약문_및_레포트_생성/valid\",\"literature/20per/*.json\")) :\n",
    "    with open(f_name, 'r', encoding = 'utf-8') as f :\n",
    "        json_data = json.load(f)\n",
    "        passages.append(json_data['Meta(Refine)']['passage'])\n",
    "        summaries.append(json_data['Annotation']['summary1'])\n",
    "        \n",
    "## 한자 제거\n",
    "passages = [re.sub(\"\\([^\\(\\)]+\\)\", \"\", passage) for passage in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072fcdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid / Test 데이터 분리\n",
    "valid_df = pd.DataFrame([ x for x in zip(summaries, passages)], columns = ['input_texts','target_texts'])\n",
    "test_df = valid_df[600:]\n",
    "valid_df = valid_df[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abecb5",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101c95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# To CSV\n",
    "train_df.to_csv(\"train_data.csv\", index = False)\n",
    "valid_df.to_csv(\"valid_data.csv\", index = False)\n",
    "test_df.to_csv(\"test_data.csv\", index = False)\n",
    "\n",
    "# To Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e6dd00",
   "metadata": {},
   "source": [
    "**Data 살펴보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab870d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train ]\n",
      "\n",
      "input :  평목이 깊게 잠들길 기다렸던 조신은 평목의 입을 막아야겠다는 생각에 목을 매어 죽이기로 했다.\n",
      "\n",
      "target :   \"어리기는 열 다섯 살이 어려요?\" 평목의 눈이 빛났다. 조신은 한 번 더 동이덩이 같은 것이 치미는 것을 삼켜버렸다. \"자, 인제 늦었으니 잡시다. 내일 마누라하고도 의논해서 좋도록 하십시 다.\" 조신은 이렇게 말하고 자리에 누웠다. 평목도 누웠다. 조신은 잠이 들지 아니하였다. 헛코를 골면서 평목이 하는 양을 엿보았다.\n",
      "평목은 잠이 드는 모양이었다. 평목이 코를 고는 것을 보고야 조신은 마음을 놓았다. 평목이 깊이 잠이 들기를 기다려서 조신은 소리 아니 나게 일어났다. \"암만해도 평목의 입을 막아놓아야 할 것이다.\" 조신은 이렇게 생각하고 구석에 놓인 도끼를 생각하였으나 방과 몸에 피가 묻어서 형적이 남을 것을 생각하고는 목을 매어 죽이기로 하였다. 조신은 손에 맞는 끈을 생각하다가 허리띠를 끌렀다. 평목이 꿈을 꾸는지 무슨 소리를 지절거리며 돌아 누웠다. 조신은 죽온 듯이 가만히 있었다. 그러나 평목이 움직이는 것을 보고는 죽 이는 것이 무서워졌다. <사람을 죽이다니.> 하고 조신은 진저리를 쳤다. 그렇지마는 평목을 살려두고는 조신 제 몸이 온전할 수가 없었다. 평목에 게 딸을 주기는 싫었다. 딸 거울보고는 아비는 아니 닮고 어미를 닮아서 어 여뻤다. 그러한 딸을 능구렁이 같은 평목에게 준다는 것은 차마 못 할 일이 었다. 그뿐 아니다. 설사 딸을 평목에게 주더라도 그것만으로 평목이 가만 있을 것 같지 아니하였다. 필시 재물도 달라고 할 것이다. 딸을 주고 재물을 주 면 조신의 복락은 다 깨어져버리고 말 것이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[ Train ]')\n",
    "print('\\ninput : ', train_dataset['input_texts'][500])\n",
    "print('\\ntarget : ', train_dataset['target_texts'][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f28eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid ]\n",
      "\n",
      "input :  이 서방의 전처가 칼에 맞아 죽었다는 유리의 말에 어머니도 몸서리쳤다.\n",
      "\n",
      "target :  “응! 이 서방의 전처가 아즉 죽지 않았다니…… 자, 얼굴을 들고 속 시원 하게 말을 좀 하려무나.” 유리는 그대로 얼굴을 들지 않았다. 울음을 끈치지 않으며, “네, 살았어요. 어젯밤에 저의 든 여관에 들었어요.” “너희와 한 여관에, 한 여관에.” 라고, 뇌일 뿐이러니 점점 사건이 여간 중대치 않은 데 짐작이나 선 모양이 다.\n",
      "“그러면 너는 이 서방의 안해가 아니로구나. 아아, 아아, 어제 혼례를 마 치고 기뻐했더니만. 그렇다면 우는 것도 괴이찮다. …… 이 애, 이 애, 인제 울지 말고…….” “어머니는 아즉도 다 모르셔요, 어젯밤에 그 안해란 여자가 별안간에 또 죽었답니다.” 어머니는 한참 생각을 돌리는 모양이더니, “죽었어? 이상한 일도 있다. 죽었다면 세상에 웃음거리가 되지 않도록, 곧 혼례를 다시 지내고……. 그 일은 입 밖에 내지 못하게 하고.” 유리는 몸서리를 치며, “그냥 죽은 게 아녜요. 맞아 죽었어요. 칼에 맞아…….” 어머니도 몸서리를 쳤다.\n",
      "“맞아 죽었어?” 그때에야 유리는 어머니의 가슴에서 떠나 교의에 몸을 던지었다.\n",
      "“어머니, 어머니!” 하고는 다시 쓰러져서 진저리를 치며 운다.\n",
      "“어머니는 아즉도 다 모르셔요.” 어머니는 번개같이 딸의 말뜻이 어데 있는지 깨달을 수 있었다.\n",
      "“이 애, 너 그게 무슨 말이냐? 슬픔에 겨워서 네가 미쳤나 부다.” “그래요, 제가 미쳤습니다. 미쳤습니다. 어머니, 저는 고만 죽고 싶어 요.” “그렇잖다, 그럴 리는 만무하다. 의심하는 네가 그르다. 누가 무에라고 해도 이 서방은 그럴 사람이 아니다. 아니고 말고……. 그런 무서운 의심을 종작 없이…….” 유리는 또 한 번 재우쳤다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[ Valid ]')\n",
    "print('\\ninput : ', valid_dataset['input_texts'][500])\n",
    "print('\\ntarget : ', valid_dataset['target_texts'][500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640e0874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Test ]\n",
      "\n",
      "input :  양은 균정을 모시고 자라 균정이 어느모로 뜯어보아도 부족한 데가 없는 인물로 후계왕에 가장 적임자라고 굳게 믿는다.\n",
      "\n",
      "target :  양은 균정과 가까이 살았다. 어렸을때부터 균정을 모시며 자랐으니만치 균정의 위인을 잘 안다. 대행왕께 적왕자가 있었으면 다른 말 쓸데 없지만, 그렇지 못하면 균정이야말로 후계왕으로 가장 적임자라고 양은 굳게 믿는다. 그 인품, 인격― 천 년에 가까운 이 사직을 물려받을 중 한 자리라, 소홀히 결정하였다가는 큰일이다.\n",
      "양이 잘 아는 바, 균정은 어느모로 뜯어보아도 추호 부족한 데가 없는 인 물이다. 더우기 만약 균정이 그 자리에 아니 오르면 당연한 순서로 거기 오 를 사람인 김제륭은 사람이 약하고 게다가 좀 경망하였다. 임금이 되기에는 부족하였다.\n",
      "만약 균정이라는 사람이 없고, 제륭 단 혼자면 세 부득이하지만, 균정이라 는 훌륭한 적임자가 있고야, 왜 부족한 이를 위에 모시랴.\n",
      "그렇게 되면 그것은 국가의 괴변이다. 이런 괴변이 생기지 않도록 사전에 방지하기 위하여, 양은, 황황히 국상 중의 서울로 달려온 것이었다.\n",
      "달려와서는 그래도 좀 주저하는 균정을 등 밀다시피 해서 적판궁 으로 들여 모시었다. 일가 병사들로 숙위케 하였다.\n",
      "일이 이렇게 되매, 경쟁자인 파에서도 가만 있지 않았다. 군사를 풀어 적 판궁을 둘러쌌다.\n",
      "양은 여기 오산을 한 것이다. 균정을 대궐로 모시어 즉위케 하고, 국왕의 명으로 호령하면, 그 뒤는 일이 순순히 될 줄로 믿었다. 국왕의 명 령에 거역하면 이는 즉 반역이니까…. 그렇기 때문에 충분한 병력의 준비도 없이 즉위의절차부터 하였던 것이었다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('[ Test ]')\n",
    "print('\\ninput : ', test_dataset['input_texts'][500])\n",
    "print('\\ntarget : ', test_dataset['target_texts'][500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ad703",
   "metadata": {},
   "source": [
    "**Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f404f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 입력할 수 있도록 전처리를 수행하는 함수\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"input_texts\"]]\n",
    "    \n",
    "    # 토큰화\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target_texts\"], max_length=max_target_length, truncation=True,padding='max_length')\n",
    "    \n",
    "    # 토큰화를 수행한 타겟 데이터를 모델 입력에 추가\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec43f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력과 타겟 문장 토큰의 최대 길이 지정\n",
    "max_input_length = 64\n",
    "max_target_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a19975e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x000002012F4BD798> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.97ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.51ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.01ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 9600\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 600\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 600\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# map() function speeds up tokenization\n",
    "train_data =train_dataset.map(preprocess_function, batched=True).remove_columns([\"input_texts\",\"target_texts\",\"token_type_ids\"])\n",
    "valid_data =valid_dataset.map(preprocess_function, batched=True).remove_columns([\"input_texts\",\"target_texts\",\"token_type_ids\"])\n",
    "test_data =test_dataset.map(preprocess_function, batched=True).remove_columns([\"input_texts\",\"target_texts\",\"token_type_ids\"])\n",
    "\n",
    "\n",
    "print(train_data)\n",
    "print(valid_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525ced3",
   "metadata": {},
   "source": [
    "### 모델 훈련 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c10fb",
   "metadata": {},
   "source": [
    "**Arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80df4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./kobart_bertscore_epoch-3\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53fb172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\n",
    "\n",
    "# DataCollator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8900779",
   "metadata": {},
   "source": [
    "**Metric Examples**\n",
    "\n",
    "[Huggingface : evaluate-metric](https://huggingface.co/evaluate-metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16ab0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, bertscore, bleu, bleurt, brier_score, cer, chrf, code_eval, comet, competition_math, coval, cuad, exact_match, f1, frugalscore, glue, google_bleu, indic_glue, mae, mahalanobis, matthews_correlation, mauve, mean_iou, meteor, mse, pearsonr, perplexity, poseval, precision, recall, rl_reliability, roc_auc, rouge, sacrebleu, sari, seqeval, spearmanr, squad, squad_v2, super_glue, ter, trec_eval, wer, wiki_split, xnli, xtreme_s, Felipehonorato/my_metric, GMFTBY/dailydialog_evaluate, GMFTBY/dailydialogevaluate, KevinSpaghetti/accuracyk, NikitaMartynov/spell-check-metric, NimaBoscarino/weat, Ochiroo/rouge_mn, Vertaix/vendiscore, Viona/infolm, Vlasta/pr_auc, abdusahmbzuai/aradiawer, abidlabs/mean_iou, abidlabs/mean_iou2, angelina-wang/directional_bias_amplification, cakiki/ndcg, codeparrot/apps_metric, cpllab/syntaxgym, daiyizheng/valid, erntkn/dice_coefficient, giulio98/code_eval_outputs, gnail/cosine_similarity, gorkaartola/metric_for_tp_fp_samples, hack/test_metric, idsedykh/codebleu, idsedykh/codebleu2, idsedykh/megaglue, idsedykh/metric, jordyvl/ece, jzm-mailchimp/joshs_second_test_metric, kaggle/ai4code, kaggle/amex, kashif/mape, kasmith/woodscore, kyokote/my_metric2, loubnabnl/apps_metric2, lvwerra/bary_score, lvwerra/test, mfumanelli/geometric_mean, mgfrantz/roc_auc_macro, ola13/precision_at_k, ronaldahmed/nwentfaithfulness, yulong-me/yl_metric, yzha/ctc_eval\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 제공 평가 지표 목록\n",
    "from datasets import list_metrics\n",
    "\n",
    "metrics_list = list_metrics()\n",
    "print(', '.join(metric for metric in metrics_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcca7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ BERTScore ]\n",
      "{'precision': [0.938406229019165, 1.0], 'recall': [0.938406229019165, 1.0], 'f1': [0.938406229019165, 1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.11(hug_trans=4.21.2)'}\n"
     ]
    }
   ],
   "source": [
    "# Metric usage example : BERTScore\n",
    "from evaluate import load\n",
    "\n",
    "predictions = [\"hello here\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "bertscore = load(\"bertscore\")\n",
    "bert_result = bert_score.compute(predictions=predictions, references=references, lang='en')\n",
    "\n",
    "print(\"[ BERTScore ]\")\n",
    "print(bert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8c48d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ BLEU ]\n",
      "{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 7, 'reference_length': 7}\n"
     ]
    }
   ],
   "source": [
    "# Metric usage example : BLEU with NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [[\"hello there general kenobi\"], [\"foo bar foobar\"]]\n",
    "bleu = load(\"bleu\")\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references, tokenizer=word_tokenize)\n",
    "\n",
    "print(\"[ BLEU ]\")\n",
    "print(bleu_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b775ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Rouge ]\n",
      "{'rouge1': 0.75, 'rouge2': 0.5, 'rougeL': 0.75, 'rougeLsum': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Metric usage example : ROUGE\n",
    "predictions = [\"hello here\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "rouge = load(\"rouge\")\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"[ Rouge ]\")\n",
    "print(rouge_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d01ce5",
   "metadata": {},
   "source": [
    "**Metric**\n",
    "\n",
    "학습 시에는 BERTScore만을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97fc6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c6d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer에 전달할 평가 함수\n",
    "import numpy\n",
    "\n",
    "def compute_metrics(eval_preds) :\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple) :\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decoding\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) # 생성 결과 텍스트 디코딩\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) # 레이블 내 -100 교체\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # 레이블 텍스트 디코딩\n",
    "\n",
    "     # Processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    # BERTScore 계산\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,lang=\"en\")\n",
    "\n",
    "    # Median 값 추출\n",
    "    result = {key : np.median(value) for key, value in result.items() if type(value) == list}\n",
    "    result = {k : round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e6ee892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = valid_data,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adea56dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\torch-py38\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 9600\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7200' max='7200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7200/7200 27:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.400600</td>\n",
       "      <td>3.293159</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.827800</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.274300</td>\n",
       "      <td>3.248249</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>0.825200</td>\n",
       "      <td>0.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.219300</td>\n",
       "      <td>3.236865</td>\n",
       "      <td>0.897700</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>0.862900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-500\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-500\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-1000\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-1500\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-2000\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-large/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-2500\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-3000\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-3500\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-4000\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-2500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-4500\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-4500\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-4500\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-5000\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-5000\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-5500\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-5500\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-5500\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-6000\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-6000\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-6000\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-6500\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-6500\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-6500\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./kobart_bertscore_epoch-3\\checkpoint-7000\n",
      "Configuration saved in ./kobart_bertscore_epoch-3\\checkpoint-7000\\config.json\n",
      "Model weights saved in ./kobart_bertscore_epoch-3\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./kobart_bertscore_epoch-3\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in ./kobart_bertscore_epoch-3\\checkpoint-7000\\special_tokens_map.json\n",
      "Deleting older checkpoint [kobart_bertscore_epoch-3\\checkpoint-5500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7200, training_loss=3.3520824347601996, metrics={'train_runtime': 1647.227, 'train_samples_per_second': 17.484, 'train_steps_per_second': 4.371, 'total_flos': 1097525624832000.0, 'train_loss': 3.3520824347601996, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd65708",
   "metadata": {},
   "source": [
    "### 테스트 데이터 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64826f85",
   "metadata": {},
   "source": [
    "**테스트 데이터 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badaf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('./test_data.csv')\n",
    "test_inputs = test_df['input_texts'].tolist()\n",
    "test_outputs = test_df['target_texts'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1494e9e",
   "metadata": {},
   "source": [
    "**모델 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d6bb99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/tokenizer.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\f94202e1dad4fcfcb282aff4c6865b6119e03c87c6fa9e5886abe93835c41ecd.dc2013f8bbecd755468e2c44397f53dc624be5451d0190744397caf61a20383f\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/added_tokens.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\7c75331e2f4b5767db997fbb489f1408eb36a3217beb3057ae8d04bd2b3f97ba.04312f398a3bbda664297588800a86e0fda9d4ef4f0749cd9d96f88043daad39\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/special_tokens_map.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\a87d2ed77831bb40ce806a97c04126addf5ecc82b3e23ecf916b2a4acdb9c29a.c23d5e62137984cf842a885705037b25b156747d145406702932d5f5d5e7c88e\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file ./kobart_bertscore_epoch-3/checkpoint-7000\\config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file ./kobart_bertscore_epoch-3/checkpoint-7000\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./kobart_bertscore_epoch-3/checkpoint-7000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "# KoBART\n",
    "test_tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2')\n",
    "test_model = BartForConditionalGeneration.from_pretrained('./kobart_bertscore_epoch-3/checkpoint-7000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43970f96",
   "metadata": {},
   "source": [
    "**문장 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0a72260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 600/600 [2:32:57<00:00, 15.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_outputs = []\n",
    "for i, test_input in enumerate(tqdm(test_inputs)) :\n",
    "    input_ids = test_tokenizer.encode(test_input, return_tensors='pt')\n",
    "    gen_ids = test_model.generate(input_ids,\n",
    "                                 do_sample = True,\n",
    "                                 max_length = 512,\n",
    "                                 min_length = 64,\n",
    "                                 repetition_penalty = 1.5,\n",
    "                                 no_repeat_ngram_size = 3,\n",
    "                                 temperature = 0.9,\n",
    "                                 top_k = 50,\n",
    "                                 top_p = 1.0)\n",
    "    generated = test_tokenizer.decode(gen_ids[0])\n",
    "    test_outputs.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb599c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Dataframe\n",
    "generated_df = pd.DataFrame([ x for  x in zip(test_inputs, test_outputs)], columns=[\"test_input\",\"test_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b3406e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df.to_csv(\"generated.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f0d066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input :  찬도는 괴상한 물건에 의해 정신이 어지러워져 온 힘을 다하려 수습해 보려 했으나 정신을 차리지 못하고 약 10분이 자나서야 정신을 차렸다.\n",
      "\n",
      "output :  </s> 찬도는 이즈음에 괴상한 물건에 의하여 정신이 어지러워졌으나 이를 수습해 보려니 별안간 정신을 차릴 수가 없었다.\n",
      "얼마 뒤 기침 소리가 났을 때였는고. “일어나서 불을 지펴 주시오?” 고 누가 물었다. 그리고 난데없는 외침을 일삼아 선뜻 좇아가지 않았다. 간혹 무슨 이야기를 하여도 알아들으려 하지 않는다. 그러나 그 모든 이야기가 다 거짓이라고 얼른 믿을 수는 없는 노릇이었다.\n",
      "찬도의 가슴은 더 끓기 시작하였다. 그는 정신으로 다시 자시질 않으니 어찌 할 수 없 기도 했다.\n",
      "남의 앞길을 막으려면 어떻게 해서 해야할는지.......\n",
      "그렇게 말을 하고자 하면 안될 것이나, 어느 틈엔가 발작이 일어나서 그의 귀를 누르고 서 있는 사람이 있었 다. 또 한 사람에게는 아주 무서운 불길이 솟았다. 그때, 남한테 당한 듯이 몹시 혼잣말같이 들리는 소리를 들었으나, 아랫목에서부터 부르는 호령을 들을 때, 그것은 너무도 큰 소리로 부르짖 는 소리였다. 아무리 소리에 놀라도, 목에서는 피가 줄줄 흐르다가 조금 걷잡힐 까닭 없이 토막을 내 버렸다. 그리하여 온 힘을 다해 겨우 남은 몸을 밖으로 끌고 나가서, ‘인제는 그만하면 됐겠다!’ 생각하였다.\n",
      "3 가느다란 돌로 만든 두터운 벽장 같은 방에 형체가 없었다. 그래도 그대로 서서만 있지 않았다면 저절로 숨을 쉬기도 하였을까? 대체 누구에게 이렇게 아픈 곳이 있다는 걸 깨달으면 지금쯤 어렸을 때의 기억이 머릿속에 번득번뜩 떠오르는 것이다. 그럴 때는 벌써 몇 해 전이다. 동정의 눈으로 그를 쳐다보던 날 밤이었다. 어떤 날, 젊은 친구 집앞에서 자고 있던 조무래기와 놀이를 하던 중에 갑자기 웬 요란한 음성이 들려왔습니다.\n",
      "<pad><pad><pad><unused56><unused70><pad><unused21><unused68><unused37>어졌다.\n",
      "<pad><unused62><pad><unused26><unused92><unused42><unused72><pad><unused10><pad>거렸다.\n",
      "<unused34><pad><unused51><unused11><pad><unused15><pad><unused53><unused36><unused40><unused9><pad><pad><unused48><pad><unused85><unused43><unused14><unk><unused31><unused98><usr> 않았다.\n",
      "<unused25>ィ<unused47>긴다.\n",
      "<pad><pad><unused5><unused13><unused23><pad><unused18><unused80><unused60><unused38><unused28><pad><unused46><unused45><unused79><pad> 뿐이었다.\n",
      "<unused33><unused86><unused32> 나갔다.\n",
      " 들어왔다.\n",
      "랐다.\n",
      "렸다.\n",
      "<unused89><unused1><unused17><pad><pad>렀다.\n",
      "<unused6><unused57><unused66><unused49><unused65><unused55><pad><unused69><unused71><unused73><unused95><pad><unused94><pad> 여호와<unused76><unused52><unused7> 뿐이다.\n",
      "<unused93><unused90><pad><unused35><unused99><sys><unused82><unused83><pad> 주었다.\n",
      "<pad><unused81><unused54><unused3><unused20><unused12> 물었다.\n",
      "졌다.\n",
      "<pad><unused19><pad><pad><unused84><pad><pad>뙈<pad><unused64>리라.\n",
      "솟는다.\n",
      "<unused87><pad>린다.\n",
      "<pad><unused22><unused2><unused29><unused30><unused41> 하였다.\n",
      "<unused97><pad><unused74><pad><unused67><pad><unused4> 생각한다.\n",
      "<pad><pad> 났다.\n",
      "<pad><pad> 것입니다.\n",
      "<unused24><unused50><unused59></s>\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "print(\"\\ninput : \", generated_df.iloc[index]['test_input'])\n",
    "print(\"\\noutput : \", generated_df.iloc[index]['test_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42456340",
   "metadata": {},
   "source": [
    "**평가 점수 계산**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289f61a",
   "metadata": {},
   "source": [
    "#### [ BERTScore ]\n",
    "\n",
    "- precision: The precision for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "\n",
    "- recall: The recall for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "\n",
    "- f1: The F1 score for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96933656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at C:\\Users\\PC/.cache\\huggingface\\transformers\\0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# BERTScore\n",
    "from evaluate import load\n",
    "\n",
    "bert_score = load(\"bertscore\")\n",
    "bert_result = bert_score.compute(predictions=test_outputs, references=list(test_df['target_texts']), lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6cf295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ BERTScore ]\n",
      "{'precision': 0.6934953927993774, 'recall': 0.7104206383228302, 'f1': 0.701668381690979}\n"
     ]
    }
   ],
   "source": [
    "bert_result = {key : np.median(value) for key, value in bert_result.items() if type(value) == list}\n",
    "print(\"[ BERTScore ]\")\n",
    "print(bert_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d94e6",
   "metadata": {},
   "source": [
    "#### [ BLEU ]\n",
    "\n",
    "- BLEU’s output is always a number between 0 and 1.\n",
    "\n",
    "  This value indicates how similar the candidate text is to the reference texts,\n",
    "    \n",
    "  with values closer to 1 representing more similar texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41106872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e950f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score (w. Mecab)\n",
    "from eunjeon import Mecab\n",
    "from datasets import load_metric\n",
    "\n",
    "mecab = Mecab()\n",
    "bleu = load_metric(\"bleu\")\n",
    "\n",
    "tokenized_test_outputs = [[mecab.morphs(sen)] for sen in list(test_df['target_texts'])]\n",
    "tokenized_generated = [mecab.morphs(sen) for sen in test_outputs]\n",
    "bleu_result = bleu.compute(predictions=tokenized_generated, references=tokenized_test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94e34044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ BLEU ]\n",
      "{'bleu': 0.036876324441368256, 'precisions': [0.24055568103892086, 0.06035250278607999, 0.018990969600169716, 0.006707064270586337], 'brevity_penalty': 1.0, 'length_ratio': 1.913024353025443, 'translation_length': 491433, 'reference_length': 256888}\n"
     ]
    }
   ],
   "source": [
    "print(\"[ BLEU ]\")\n",
    "print(bleu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97647085",
   "metadata": {},
   "source": [
    "#### [[ ROUGE ]](https://dacon.io/competitions/open/235671/overview/rules)\n",
    "\n",
    "- DACON의 한국어 문서 추출요약 AI 경진대회에서 사용된 ROUGE 스코어 산식 코드를 사용하였다\n",
    "\n",
    "\n",
    "- 해당 코드는 py-rouge 소스코드를 한글에 맞게 수정하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2b00c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import platform\n",
    "import itertools\n",
    "import collections\n",
    "import pkg_resources  # pip install py-rouge\n",
    "from io import open\n",
    "\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Rouge:\n",
    "    DEFAULT_METRICS = {\"rouge-n\"}\n",
    "    DEFAULT_N = 1\n",
    "    STATS = [\"f\", \"p\", \"r\"]\n",
    "    AVAILABLE_METRICS = {\"rouge-n\", \"rouge-l\", \"rouge-w\"}\n",
    "    AVAILABLE_LENGTH_LIMIT_TYPES = {\"words\", \"bytes\"}\n",
    "    REMOVE_CHAR_PATTERN = re.compile(\"[^A-Za-z0-9가-힣]\")\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics=None,\n",
    "        max_n=None,\n",
    "        limit_length=True,\n",
    "        length_limit=1000,\n",
    "        length_limit_type=\"words\",\n",
    "        apply_avg=True,\n",
    "        apply_best=False,\n",
    "        use_tokenizer=True,\n",
    "        alpha=0.5,\n",
    "        weight_factor=1.0,\n",
    "    ):\n",
    "        self.metrics = metrics[:] if metrics is not None else Rouge.DEFAULT_METRICS\n",
    "        for m in self.metrics:\n",
    "            if m not in Rouge.AVAILABLE_METRICS:\n",
    "                raise ValueError(\"Unknown metric '{}'\".format(m))\n",
    "\n",
    "\n",
    "        self.max_n = max_n if \"rouge-n\" in self.metrics else None\n",
    "        # Add all rouge-n metrics\n",
    "        if self.max_n is not None:\n",
    "            index_rouge_n = self.metrics.index(\"rouge-n\")\n",
    "            del self.metrics[index_rouge_n]\n",
    "            self.metrics += [\"rouge-{}\".format(n) for n in range(1, self.max_n + 1)]\n",
    "        self.metrics = set(self.metrics)\n",
    "\n",
    "\n",
    "        self.limit_length = limit_length\n",
    "        if self.limit_length:\n",
    "            if length_limit_type not in Rouge.AVAILABLE_LENGTH_LIMIT_TYPES:\n",
    "                raise ValueError(\"Unknown length_limit_type '{}'\".format(length_limit_type))\n",
    "\n",
    "\n",
    "        self.length_limit = length_limit\n",
    "        if self.length_limit == 0:\n",
    "            self.limit_length = False\n",
    "        self.length_limit_type = length_limit_type\n",
    "\n",
    "\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        if use_tokenizer:\n",
    "            self.tokenizer = Mecab()\n",
    "\n",
    "\n",
    "        self.apply_avg = apply_avg\n",
    "        self.apply_best = apply_best\n",
    "        self.alpha = alpha\n",
    "        self.weight_factor = weight_factor\n",
    "        if self.weight_factor <= 0:\n",
    "            raise ValueError(\"ROUGE-W weight factor must greater than 0.\")\n",
    "\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        if self.use_tokenizer:\n",
    "            return self.tokenizer.morphs(text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def split_into_sentences(text):\n",
    "        return text.split(\"\\n\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_ngrams(n, text):\n",
    "        ngram_set = collections.defaultdict(int)\n",
    "        max_index_ngram_start = len(text) - n\n",
    "        for i in range(max_index_ngram_start + 1):\n",
    "            ngram_set[tuple(text[i : i + n])] += 1\n",
    "        return ngram_set\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_into_words(sentences):\n",
    "        return list(itertools.chain(*[_.split() for _ in sentences]))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_word_ngrams_and_length(n, sentences):\n",
    "        assert len(sentences) > 0\n",
    "        assert n > 0\n",
    "\n",
    "\n",
    "        tokens = Rouge._split_into_words(sentences)\n",
    "        return Rouge._get_ngrams(n, tokens), tokens, len(tokens) - (n - 1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unigrams(sentences):\n",
    "        assert len(sentences) > 0\n",
    "\n",
    "\n",
    "        tokens = Rouge._split_into_words(sentences)\n",
    "        unigram_set = collections.defaultdict(int)\n",
    "        for token in tokens:\n",
    "            unigram_set[token] += 1\n",
    "        return unigram_set, len(tokens)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_p_r_f_score(\n",
    "        evaluated_count,\n",
    "        reference_count,\n",
    "        overlapping_count,\n",
    "        alpha=0.5,\n",
    "        weight_factor=1.0,\n",
    "    ):\n",
    "        precision = 0.0 if evaluated_count == 0 else overlapping_count / float(evaluated_count)\n",
    "        if weight_factor != 1.0:\n",
    "            precision = precision ** (1.0 / weight_factor)\n",
    "        recall = 0.0 if reference_count == 0 else overlapping_count / float(reference_count)\n",
    "        if weight_factor != 1.0:\n",
    "            recall = recall ** (1.0 / weight_factor)\n",
    "        f1_score = Rouge._compute_f_score(precision, recall, alpha)\n",
    "        return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_f_score(precision, recall, alpha=0.5):\n",
    "        return (\n",
    "            0.0\n",
    "            if (recall == 0.0 or precision == 0.0)\n",
    "            else precision * recall / ((1 - alpha) * precision + alpha * recall)\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ngrams(evaluated_sentences, reference_sentences, n):\n",
    "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "\n",
    "        evaluated_ngrams, _, evaluated_count = Rouge._get_word_ngrams_and_length(\n",
    "            n, evaluated_sentences\n",
    "        )\n",
    "        reference_ngrams, _, reference_count = Rouge._get_word_ngrams_and_length(\n",
    "            n, reference_sentences\n",
    "        )\n",
    "\n",
    "\n",
    "        # Gets the overlapping ngrams between evaluated and reference\n",
    "        overlapping_ngrams = set(evaluated_ngrams.keys()).intersection(set(reference_ngrams.keys()))\n",
    "        overlapping_count = 0\n",
    "        for ngram in overlapping_ngrams:\n",
    "            overlapping_count += min(evaluated_ngrams[ngram], reference_ngrams[ngram])\n",
    "\n",
    "\n",
    "        return evaluated_count, reference_count, overlapping_count\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ngrams_lcs(evaluated_sentences, reference_sentences, weight_factor=1.0):\n",
    "        def _lcs(x, y):\n",
    "            m = len(x)\n",
    "            n = len(y)\n",
    "            vals = collections.defaultdict(int)\n",
    "            dirs = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i - 1] == y[j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j - 1] + 1\n",
    "                        dirs[i, j] = \"|\"\n",
    "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j]\n",
    "                        dirs[i, j] = \"^\"\n",
    "                    else:\n",
    "                        vals[i, j] = vals[i, j - 1]\n",
    "                        dirs[i, j] = \"<\"\n",
    "\n",
    "\n",
    "            return vals, dirs\n",
    "\n",
    "\n",
    "        def _wlcs(x, y, weight_factor):\n",
    "            m = len(x)\n",
    "            n = len(y)\n",
    "            vals = collections.defaultdict(float)\n",
    "            dirs = collections.defaultdict(int)\n",
    "            lengths = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i - 1] == y[j - 1]:\n",
    "                        length_tmp = lengths[i - 1, j - 1]\n",
    "                        vals[i, j] = (\n",
    "                            vals[i - 1, j - 1]\n",
    "                            + (length_tmp + 1) ** weight_factor\n",
    "                            - length_tmp ** weight_factor\n",
    "                        )\n",
    "                        dirs[i, j] = \"|\"\n",
    "                        lengths[i, j] = length_tmp + 1\n",
    "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j]\n",
    "                        dirs[i, j] = \"^\"\n",
    "                        lengths[i, j] = 0\n",
    "                    else:\n",
    "                        vals[i, j] = vals[i, j - 1]\n",
    "                        dirs[i, j] = \"<\"\n",
    "                        lengths[i, j] = 0\n",
    "\n",
    "\n",
    "            return vals, dirs\n",
    "\n",
    "\n",
    "        def _mark_lcs(mask, dirs, m, n):\n",
    "            while m != 0 and n != 0:\n",
    "                if dirs[m, n] == \"|\":\n",
    "                    m -= 1\n",
    "                    n -= 1\n",
    "                    mask[m] = 1\n",
    "                elif dirs[m, n] == \"^\":\n",
    "                    m -= 1\n",
    "                elif dirs[m, n] == \"<\":\n",
    "                    n -= 1\n",
    "                else:\n",
    "                    raise UnboundLocalError(\"Illegal move\")\n",
    "\n",
    "\n",
    "            return mask\n",
    "\n",
    "\n",
    "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "\n",
    "        evaluated_unigrams_dict, evaluated_count = Rouge._get_unigrams(evaluated_sentences)\n",
    "        reference_unigrams_dict, reference_count = Rouge._get_unigrams(reference_sentences)\n",
    "\n",
    "\n",
    "        # Has to use weight factor for WLCS\n",
    "        use_WLCS = weight_factor != 1.0\n",
    "        if use_WLCS:\n",
    "            evaluated_count = evaluated_count ** weight_factor\n",
    "            reference_count = 0\n",
    "\n",
    "\n",
    "        overlapping_count = 0.0\n",
    "        for reference_sentence in reference_sentences:\n",
    "            reference_sentence_tokens = reference_sentence.split()\n",
    "            if use_WLCS:\n",
    "                reference_count += len(reference_sentence_tokens) ** weight_factor\n",
    "            hit_mask = [0 for _ in range(len(reference_sentence_tokens))]\n",
    "\n",
    "\n",
    "            for evaluated_sentence in evaluated_sentences:\n",
    "                evaluated_sentence_tokens = evaluated_sentence.split()\n",
    "\n",
    "\n",
    "                if use_WLCS:\n",
    "                    _, lcs_dirs = _wlcs(\n",
    "                        reference_sentence_tokens,\n",
    "                        evaluated_sentence_tokens,\n",
    "                        weight_factor,\n",
    "                    )\n",
    "                else:\n",
    "                    _, lcs_dirs = _lcs(reference_sentence_tokens, evaluated_sentence_tokens)\n",
    "                _mark_lcs(\n",
    "                    hit_mask,\n",
    "                    lcs_dirs,\n",
    "                    len(reference_sentence_tokens),\n",
    "                    len(evaluated_sentence_tokens),\n",
    "                )\n",
    "\n",
    "\n",
    "            overlapping_count_length = 0\n",
    "            for ref_token_id, val in enumerate(hit_mask):\n",
    "                if val == 1:\n",
    "                    token = reference_sentence_tokens[ref_token_id]\n",
    "                    if evaluated_unigrams_dict[token] > 0 and reference_unigrams_dict[token] > 0:\n",
    "                        evaluated_unigrams_dict[token] -= 1\n",
    "                        reference_unigrams_dict[ref_token_id] -= 1\n",
    "\n",
    "\n",
    "                        if use_WLCS:\n",
    "                            overlapping_count_length += 1\n",
    "                            if (\n",
    "                                ref_token_id + 1 < len(hit_mask) and hit_mask[ref_token_id + 1] == 0\n",
    "                            ) or ref_token_id + 1 == len(hit_mask):\n",
    "                                overlapping_count += overlapping_count_length ** weight_factor\n",
    "                                overlapping_count_length = 0\n",
    "                        else:\n",
    "                            overlapping_count += 1\n",
    "\n",
    "\n",
    "        if use_WLCS:\n",
    "            reference_count = reference_count ** weight_factor\n",
    "\n",
    "\n",
    "        return evaluated_count, reference_count, overlapping_count\n",
    "\n",
    "\n",
    "    def get_scores(self, hypothesis, references):\n",
    "        if isinstance(hypothesis, str):\n",
    "            hypothesis, references = [hypothesis], [references]\n",
    "\n",
    "\n",
    "        if type(hypothesis) != type(references):\n",
    "            raise ValueError(\"'hyps' and 'refs' are not of the same type\")\n",
    "\n",
    "\n",
    "        if len(hypothesis) != len(references):\n",
    "            raise ValueError(\"'hyps' and 'refs' do not have the same length\")\n",
    "        scores = {}\n",
    "        has_rouge_n_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]) > 0\n",
    "        )\n",
    "        if has_rouge_n_metric:\n",
    "            scores.update(self._get_scores_rouge_n(hypothesis, references))\n",
    "            # scores = {**scores, **self._get_scores_rouge_n(hypothesis, references)}\n",
    "\n",
    "\n",
    "        has_rouge_l_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"l\"]) > 0\n",
    "        )\n",
    "        if has_rouge_l_metric:\n",
    "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, False))\n",
    "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, False)}\n",
    "\n",
    "\n",
    "        has_rouge_w_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"w\"]) > 0\n",
    "        )\n",
    "        if has_rouge_w_metric:\n",
    "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, True))\n",
    "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, True)}\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _get_scores_rouge_n(self, all_hypothesis, all_references):\n",
    "        metrics = [metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]\n",
    "\n",
    "\n",
    "        if self.apply_avg or self.apply_best:\n",
    "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS} for metric in metrics}\n",
    "        else:\n",
    "            scores = {\n",
    "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
    "                for metric in metrics\n",
    "            }\n",
    "\n",
    "\n",
    "        for sample_id, (hypothesis, references) in enumerate(zip(all_hypothesis, all_references)):\n",
    "            assert isinstance(hypothesis, str)\n",
    "            has_multiple_references = False\n",
    "            if isinstance(references, list):\n",
    "                has_multiple_references = len(references) > 1\n",
    "                if not has_multiple_references:\n",
    "                    references = references[0]\n",
    "\n",
    "\n",
    "            # Prepare hypothesis and reference(s)\n",
    "            hypothesis = self._preprocess_summary_as_a_whole(hypothesis)\n",
    "            references = (\n",
    "                [self._preprocess_summary_as_a_whole(reference) for reference in references]\n",
    "                if has_multiple_references\n",
    "                else [self._preprocess_summary_as_a_whole(references)]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Compute scores\n",
    "            for metric in metrics:\n",
    "                suffix = metric.split(\"-\")[-1]\n",
    "                n = int(suffix)\n",
    "\n",
    "\n",
    "                # Aggregate\n",
    "                if self.apply_avg:\n",
    "                    # average model\n",
    "                    total_hypothesis_ngrams_count = 0\n",
    "                    total_reference_ngrams_count = 0\n",
    "                    total_ngrams_overlapping_count = 0\n",
    "\n",
    "\n",
    "                    for reference in references:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                        total_hypothesis_ngrams_count += hypothesis_count\n",
    "                        total_reference_ngrams_count += reference_count\n",
    "                        total_ngrams_overlapping_count += overlapping_ngrams\n",
    "\n",
    "\n",
    "                    score = Rouge._compute_p_r_f_score(\n",
    "                        total_hypothesis_ngrams_count,\n",
    "                        total_reference_ngrams_count,\n",
    "                        total_ngrams_overlapping_count,\n",
    "                        self.alpha,\n",
    "                    )\n",
    "\n",
    "\n",
    "                    for stat in Rouge.STATS:\n",
    "                        scores[metric][stat] += score[stat]\n",
    "                else:\n",
    "                    # Best model\n",
    "                    if self.apply_best:\n",
    "                        best_current_score = None\n",
    "                        for reference in references:\n",
    "                            (\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                            score = Rouge._compute_p_r_f_score(\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                                self.alpha,\n",
    "                            )\n",
    "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
    "                                best_current_score = score\n",
    "\n",
    "\n",
    "                        for stat in Rouge.STATS:\n",
    "                            scores[metric][stat] += best_current_score[stat]\n",
    "                    # Keep all\n",
    "                    else:\n",
    "                        for reference in references:\n",
    "                            (\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                            score = Rouge._compute_p_r_f_score(\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                                self.alpha,\n",
    "                            )\n",
    "                            for stat in Rouge.STATS:\n",
    "                                scores[metric][sample_id][stat].append(score[stat])\n",
    "\n",
    "\n",
    "        # Compute final score with the average or the the max\n",
    "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
    "            for metric in metrics:\n",
    "                for stat in Rouge.STATS:\n",
    "                    scores[metric][stat] /= len(all_hypothesis)\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _get_scores_rouge_l_or_w(self, all_hypothesis, all_references, use_w=False):\n",
    "        metric = \"rouge-w\" if use_w else \"rouge-l\"\n",
    "        if self.apply_avg or self.apply_best:\n",
    "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS}}\n",
    "        else:\n",
    "            scores = {\n",
    "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
    "            }\n",
    "\n",
    "\n",
    "        for sample_id, (hypothesis_sentences, references_sentences) in enumerate(\n",
    "            zip(all_hypothesis, all_references)\n",
    "        ):\n",
    "            assert isinstance(hypothesis_sentences, str)\n",
    "            has_multiple_references = False\n",
    "            if isinstance(references_sentences, list):\n",
    "                has_multiple_references = len(references_sentences) > 1\n",
    "                if not has_multiple_references:\n",
    "                    references_sentences = references_sentences[0]\n",
    "\n",
    "\n",
    "            # Prepare hypothesis and reference(s)\n",
    "            hypothesis_sentences = self._preprocess_summary_per_sentence(hypothesis_sentences)\n",
    "            references_sentences = (\n",
    "                [\n",
    "                    self._preprocess_summary_per_sentence(reference)\n",
    "                    for reference in references_sentences\n",
    "                ]\n",
    "                if has_multiple_references\n",
    "                else [self._preprocess_summary_per_sentence(references_sentences)]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Compute scores\n",
    "            # Aggregate\n",
    "            if self.apply_avg:\n",
    "                # average model\n",
    "                total_hypothesis_ngrams_count = 0\n",
    "                total_reference_ngrams_count = 0\n",
    "                total_ngrams_overlapping_count = 0\n",
    "\n",
    "\n",
    "                for reference_sentences in references_sentences:\n",
    "                    (\n",
    "                        hypothesis_count,\n",
    "                        reference_count,\n",
    "                        overlapping_ngrams,\n",
    "                    ) = Rouge._compute_ngrams_lcs(\n",
    "                        hypothesis_sentences,\n",
    "                        reference_sentences,\n",
    "                        self.weight_factor if use_w else 1.0,\n",
    "                    )\n",
    "                    total_hypothesis_ngrams_count += hypothesis_count\n",
    "                    total_reference_ngrams_count += reference_count\n",
    "                    total_ngrams_overlapping_count += overlapping_ngrams\n",
    "\n",
    "\n",
    "                score = Rouge._compute_p_r_f_score(\n",
    "                    total_hypothesis_ngrams_count,\n",
    "                    total_reference_ngrams_count,\n",
    "                    total_ngrams_overlapping_count,\n",
    "                    self.alpha,\n",
    "                    self.weight_factor if use_w else 1.0,\n",
    "                )\n",
    "                for stat in Rouge.STATS:\n",
    "                    scores[metric][stat] += score[stat]\n",
    "            else:\n",
    "                # Best model\n",
    "                if self.apply_best:\n",
    "                    best_current_score = None\n",
    "                    best_current_score_wlcs = None\n",
    "                    for reference_sentences in references_sentences:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams_lcs(\n",
    "                            hypothesis_sentences,\n",
    "                            reference_sentences,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "                        score = Rouge._compute_p_r_f_score(\n",
    "                            total_hypothesis_ngrams_count,\n",
    "                            total_reference_ngrams_count,\n",
    "                            total_ngrams_overlapping_count,\n",
    "                            self.alpha,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "\n",
    "\n",
    "                        if use_w:\n",
    "                            reference_count_for_score = reference_count ** (\n",
    "                                1.0 / self.weight_factor\n",
    "                            )\n",
    "                            overlapping_ngrams_for_score = overlapping_ngrams\n",
    "                            score_wlcs = (\n",
    "                                overlapping_ngrams_for_score / reference_count_for_score\n",
    "                            ) ** (1.0 / self.weight_factor)\n",
    "\n",
    "\n",
    "                            if (\n",
    "                                best_current_score_wlcs is None\n",
    "                                or score_wlcs > best_current_score_wlcs\n",
    "                            ):\n",
    "                                best_current_score = score\n",
    "                                best_current_score_wlcs = score_wlcs\n",
    "                        else:\n",
    "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
    "                                best_current_score = score\n",
    "\n",
    "\n",
    "                    for stat in Rouge.STATS:\n",
    "                        scores[metric][stat] += best_current_score[stat]\n",
    "                # Keep all\n",
    "                else:\n",
    "                    for reference_sentences in references_sentences:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams_lcs(\n",
    "                            hypothesis_sentences,\n",
    "                            reference_sentences,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "                        score = Rouge._compute_p_r_f_score(\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                            self.alpha,\n",
    "                            self.weight_factor,\n",
    "                        )\n",
    "\n",
    "\n",
    "                        for stat in Rouge.STATS:\n",
    "                            scores[metric][sample_id][stat].append(score[stat])\n",
    "\n",
    "\n",
    "        # Compute final score with the average or the the max\n",
    "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
    "            for stat in Rouge.STATS:\n",
    "                scores[metric][stat] /= len(all_hypothesis)\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _preprocess_summary_as_a_whole(self, summary):\n",
    "        sentences = Rouge.split_into_sentences(summary)\n",
    "\n",
    "\n",
    "        # Truncate\n",
    "        if self.limit_length:\n",
    "            # By words\n",
    "            if self.length_limit_type == \"words\":\n",
    "                summary = \" \".join(sentences)\n",
    "                all_tokens = summary.split()  # Counting as in the perls script\n",
    "                summary = \" \".join(all_tokens[: self.length_limit])\n",
    "\n",
    "\n",
    "            # By bytes\n",
    "            elif self.length_limit_type == \"bytes\":\n",
    "                summary = \"\"\n",
    "                current_len = 0\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    sentence_len = len(sentence)\n",
    "\n",
    "\n",
    "                    if current_len + sentence_len < self.length_limit:\n",
    "                        if current_len != 0:\n",
    "                            summary += \" \"\n",
    "                        summary += sentence\n",
    "                        current_len += sentence_len\n",
    "                    else:\n",
    "                        if current_len > 0:\n",
    "                            summary += \" \"\n",
    "                        summary += sentence[: self.length_limit - current_len]\n",
    "                        break\n",
    "        else:\n",
    "            summary = \" \".join(sentences)\n",
    "\n",
    "\n",
    "        summary = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary.lower()).strip()\n",
    "\n",
    "\n",
    "        tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary))\n",
    "        preprocessed_summary = [\" \".join(tokens)]\n",
    "\n",
    "\n",
    "        return preprocessed_summary\n",
    "\n",
    "\n",
    "    def _preprocess_summary_per_sentence(self, summary):\n",
    "        sentences = Rouge.split_into_sentences(summary)\n",
    "\n",
    "\n",
    "        # Truncate\n",
    "        if self.limit_length:\n",
    "            final_sentences = []\n",
    "            current_len = 0\n",
    "            # By words\n",
    "            if self.length_limit_type == \"words\":\n",
    "                for sentence in sentences:\n",
    "                    tokens = sentence.strip().split()\n",
    "                    tokens_len = len(tokens)\n",
    "                    if current_len + tokens_len < self.length_limit:\n",
    "                        sentence = \" \".join(tokens)\n",
    "                        final_sentences.append(sentence)\n",
    "                        current_len += tokens_len\n",
    "                    else:\n",
    "                        sentence = \" \".join(tokens[: self.length_limit - current_len])\n",
    "                        final_sentences.append(sentence)\n",
    "                        break\n",
    "            # By bytes\n",
    "            elif self.length_limit_type == \"bytes\":\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    sentence_len = len(sentence)\n",
    "                    if current_len + sentence_len < self.length_limit:\n",
    "                        final_sentences.append(sentence)\n",
    "                        current_len += sentence_len\n",
    "                    else:\n",
    "                        sentence = sentence[: self.length_limit - current_len]\n",
    "                        final_sentences.append(sentence)\n",
    "                        break\n",
    "            sentences = final_sentences\n",
    "\n",
    "\n",
    "        final_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence.lower()).strip()\n",
    "\n",
    "\n",
    "            tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence))\n",
    "\n",
    "\n",
    "            sentence = \" \".join(tokens)\n",
    "\n",
    "\n",
    "            final_sentences.append(sentence)\n",
    "\n",
    "\n",
    "        return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bbf4bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RougeScorer:\n",
    "    def __init__(self):\n",
    "        self.rouge_evaluator = Rouge(\n",
    "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
    "            max_n=2,\n",
    "            limit_length=True,\n",
    "            length_limit=1000,\n",
    "            length_limit_type=\"words\",\n",
    "            use_tokenizer=True,\n",
    "            apply_avg=True,\n",
    "            apply_best=False,\n",
    "            alpha=0.5,  # Default F1_score\n",
    "            weight_factor=1.2,\n",
    "        )\n",
    "\n",
    "    def compute_rouge(self, ref_df, hyp_df):\n",
    "        #ref_df = pd.read_csv(ref_path)\n",
    "        #hyp_df = pd.read_csv(hyp_path)\n",
    "        hyp_df.iloc[:,1] = hyp_df.iloc[:,1].fillna(' ')\n",
    "        ids = ref_df['id']\n",
    "        hyp_df = hyp_df[hyp_df['id'].isin(ids)]\n",
    "        hyp_df.index = ref_df.index\n",
    "\n",
    "        ref_df = ref_df.sort_values(by=[\"id\"])\n",
    "        hyp_df = hyp_df.sort_values(by=[\"id\"])\n",
    "        ref_df[\"id\"] = ref_df[\"id\"].astype(int)\n",
    "        hyp_df[\"id\"] = hyp_df[\"id\"].astype(int)\n",
    "\n",
    "        hyps = [tuple(row) for row in hyp_df.values]\n",
    "        refs = [tuple(row) for row in ref_df.values]\n",
    "\n",
    "        reference_summaries = []\n",
    "        generated_summaries = []\n",
    "\n",
    "        for ref_tp, hyp_tp in zip(refs, hyps):\n",
    "            ref, ref_id = ref_tp\n",
    "            hyp, hyp_id = hyp_tp\n",
    "\n",
    "            assert ref_id == hyp_id\n",
    "\n",
    "            reference_summaries.append(ref)\n",
    "            generated_summaries.append(hyp)\n",
    "\n",
    "        scores = self.rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n",
    "        str_scores = self.format_rouge_scores(scores)\n",
    "        #self.save_rouge_scores(str_scores)\n",
    "        return str_scores\n",
    "\n",
    "    def save_rouge_scores(self, str_scores):\n",
    "        with open(\"rouge_scores.txt\", \"w\") as output:\n",
    "            output.write(str_scores)\n",
    "\n",
    "    def format_rouge_scores(self, scores):\n",
    "    \treturn \"{:.3f},{:.3f},{:.3f}\".format(\n",
    "            scores[\"rouge-1\"][\"f\"],\n",
    "            scores[\"rouge-2\"][\"f\"],\n",
    "            scores[\"rouge-l\"][\"f\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4f71f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE\n",
    "rouge = RougeScorer()\n",
    "\n",
    "rouge_target = pd.DataFrame(list(test_df['target_texts']), columns=['summary'])\n",
    "rouge_target['id'] = rouge_target.index\n",
    "rouge_generated = pd.DataFrame(test_outputs, columns=['summary'])\n",
    "rouge_generated['id'] = rouge_generated.index\n",
    "\n",
    "rouge_result = rouge.compute_rouge(rouge_target,rouge_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "670aa5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Rouge ]\n",
      "Rouge-1, Rouge-2, Rouge-L :  0.339,0.072,0.194\n"
     ]
    }
   ],
   "source": [
    "print(\"[ Rouge ]\")\n",
    "print(\"Rouge-1, Rouge-2, Rouge-L : \", rouge_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb7209",
   "metadata": {},
   "source": [
    "### 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "521d09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original :  하얀 케이크를 자르고 있는 여자가 있다.\n",
      "generated :  </s> “누님!” 하고 여자는 소리쳤다. 그의 옆에 서 있던 여자가 벌떡 일어나며 두 손을 번쩍 들고 하얀 케이크를 자르고 있었다. 그 여자의 얼굴에는 이 런 일이 생 각이 났었다. 흰 옷도 입으신 듯싶었 다. “그리고 무슨 장난일까? 나는 퍽 오래전부터 그런 생각을 해 왔어요. 요즈음 밤마다 새까만 옷을 입고 다니는 것 같습니다. 어째서 그러우? 왜 그럴 때에 날쌔게 안 나타났을까. 내가 아무리 못 본 체해도 알 수 있나요. 그리고 또 어떤 방에서든지 나 같은 놈은 언제나 나와 같이 놀다가 가겠어요..... 그러나.......\n",
      "저를 데리고 있는 사람이 누구냐? 이런 질문과 저편을 보고 웃으면서 물으니까, 우리 모두 참말 이상하리만큼 마음이 가라앉아 있답니다. 저는 물론 당신이 얼마나 똑똑하신 분인가요? 제가 만일 당신의 꿈을 이루지 못하면 당신에게 무한한 행복을 줄 것은 없습니다마는...... 선생님께서 그리시는 것을 꼭 한 가지 들어 주십시요 그래서 아유. 아까는 아무튼 잘됐어요, 그러니까 제발 저를 좀 맡아 보구려 이렇게 말하셨더니, 그럼 당신은 정말 그렇게 하세요 하셨다 고 말씀하시더라고 하시던 것입니다. 그런데 오늘 와서는 내 생각으로는 나도 모르게 영감을</s>\n"
     ]
    }
   ],
   "source": [
    "text = '하얀 케이크를 자르고 있는 여자가 있다.'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').cuda()\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         do_sample=True,\n",
    "                         max_length=256,\n",
    "                         min_length=16,\n",
    "                         repetition_penalty=1.5,\n",
    "                         no_repeat_ngram_size=3,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.92)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print('original : ', text)\n",
    "print('generated : ', generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
