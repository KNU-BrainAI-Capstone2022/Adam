{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c025ad7",
   "metadata": {},
   "source": [
    "### 테스트 데이터 평가\n",
    "\n",
    "SKT에서 기본적으로 제공하는 KoBART 모델로 Story Generation을 수행하고 평가하여 우리가 학습시킨 KoBART_story 모델과 성능을 비교한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7f3e8c",
   "metadata": {},
   "source": [
    "**테스트 데이터 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f32af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('./test_data.csv')\n",
    "test_inputs = test_df['input_texts'].tolist()\n",
    "test_outputs = test_df['target_texts'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d3f86",
   "metadata": {},
   "source": [
    "**모델 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037c1e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "test_tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-base-v2')\n",
    "test_model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186b007",
   "metadata": {},
   "source": [
    "**문장 생성하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c115ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 600/600 [2:20:09<00:00, 14.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "generated_outputs = []\n",
    "for i, test_input in enumerate(tqdm(test_inputs)) :\n",
    "    input_ids = test_tokenizer.encode(test_input, return_tensors='pt')\n",
    "    gen_ids = test_model.generate(input_ids,\n",
    "                                 do_sample = True,\n",
    "                                 max_length = 512,\n",
    "                                 min_length = 64,\n",
    "                                 repetition_penalty = 1.5,\n",
    "                                 no_repeat_ngram_size = 3,\n",
    "                                 temperature = 0.9,\n",
    "                                 top_k = 50,\n",
    "                                 top_p = 1.0)\n",
    "    generated = test_tokenizer.decode(gen_ids[0])\n",
    "    generated_outputs.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b01dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Dataframe\n",
    "generated_df = pd.DataFrame([ x for  x in zip(test_inputs, test_outputs)], columns=[\"test_input\",\"generated_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f6f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To CSV File\n",
    "generated_df.to_csv(\"generated-kobart-base.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614c4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input :  찬도는 괴상한 물건에 의해 정신이 어지러워져 온 힘을 다하려 수습해 보려 했으나 정신을 차리지 못하고 약 10분이 자나서야 정신을 차렸다.\n",
      "\n",
      "output :   피! 그것은 과연 괴상한 물건이었다. 거기에 커다란 충동을 받은 찬도는 정신을 못 차리고 서 있었다. 그는 온 힘을 다하여 정신을 수습해 보려하였 다. 그러나 애쓰면 애쓸 수록 정신은 더욱 어지러워졌다. 10여 분이 지난 뒤에야 그는 펄떡 정신을 차렸다. 동시에, 아직껏 불같이 울어대는 어린아이가 있는 것을 처음으로 인식하였다. 그는, 발로써, 그 어 린아이를 힘껏 찼다. 그리고 도망하려 뛰어나왔다. 그러나 그는, 그 집 문밖에서 순사에게 잡혔다. 순사는 어린아이가 너무도 우는 것이 수상하여, 그 집 문밖에서 동정을 엿듣고 있었던 것이었다. 이렇게 그 찬도의 경력을 이야기해오던 전 판사는, 잠깐 말을 멈추었다가 다시 이었다. 그런데, 그 사람에게는 제 말과 같이 공범이 있었는데, 예심 조서 이하 초심 기록 아무 데를 보든 다 저 혼자 범행한 모 양으로 됐지 공범의 이야기는 없거든요. 하기는 경찰서의 조사에는 잠깐 그 런 이야기가 있었지만, 그뒤부터는 늘 그것을 부인해왔어요. 그런데 그 이 유로 그 사람이 제 입으로 내게 한 말을 듣자면, 자기는 아무 세상 철없는 순진한 어린애를 죽였으니깐 죽어도 싼데…… 자기는 어차피 사형될 터에, 공연히 남까지 끌어 넣어서 그 사람까지 죽이면 무얼 하느냐고…… 그 사람, 공범의 죄를 보자면 역시 열 번 죽여도 싸기는 하지만 그 사람에게는 처자 가 있는 것을 생각하니깐, 자기의 어렸을 때의 생각이 나서 차마 불어 넣지 못하겠다고요. ‘피’ 를 본다 하는 것은 과연 무서운 것이외다. 아직껏 아 무 반성 없이 온갖 죄악을 범해오던 그 사람도, 뜻하지 않은 피를 보고 그 만 양심이 일어서면서 동시에 그런 고찰도 생긴 모양이지요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "print(\"\\ninput : \", generated_df.iloc[index]['test_input'])\n",
    "print(\"\\noutput : \", generated_df.iloc[index]['generated_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fdc176",
   "metadata": {},
   "source": [
    "### 평가 점수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d05d2c",
   "metadata": {},
   "source": [
    "#### [ BERTScore ]\n",
    "\n",
    "- precision: The precision for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "\n",
    "- recall: The recall for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n",
    "\n",
    "\n",
    "- f1: The F1 score for each sentence from the predictions + references lists, which ranges from 0.0 to 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8daa5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore\n",
    "from evaluate import load\n",
    "\n",
    "bert_score = load(\"bertscore\")\n",
    "bert_result = bert_score.compute(predictions=generated_outputs, references=test_outputs, lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14b55d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ BERTScore ]\n",
      "{'precision': 0.6828727424144745, 'recall': 0.6413138508796692, 'f1': 0.6618437170982361}\n"
     ]
    }
   ],
   "source": [
    "bert_result = {key : np.median(value) for key, value in bert_result.items() if type(value) == list}\n",
    "print(\"[ BERTScore ]\")\n",
    "print(bert_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f6cbf",
   "metadata": {},
   "source": [
    "#### [ BLEU ]\n",
    "\n",
    "- BLEU’s output is always a number between 0 and 1.\n",
    "\n",
    "  This value indicates how similar the candidate text is to the reference texts,\n",
    "    \n",
    "  with values closer to 1 representing more similar texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1caf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU Score (w. Mecab)\n",
    "from eunjeon import Mecab\n",
    "from datasets import load_metric\n",
    "\n",
    "mecab = Mecab()\n",
    "bleu = load_metric(\"bleu\")\n",
    "\n",
    "tokenized_test_outputs = [[mecab.morphs(sen)] for sen in test_outputs]\n",
    "tokenized_generated = [mecab.morphs(sen) for sen in generated_outputs]\n",
    "bleu_result = bleu.compute(predictions=tokenized_generated, references=tokenized_test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b75e3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ BLEU ]\n",
      "{'bleu': 0.047741959826895775, 'precisions': [0.25758071507473745, 0.06364132946762469, 0.02475412826016808, 0.01280267577329005], 'brevity_penalty': 1.0, 'length_ratio': 1.3919879480551836, 'translation_length': 357585, 'reference_length': 256888}\n"
     ]
    }
   ],
   "source": [
    "print(\"[ BLEU ]\")\n",
    "print(bleu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f985b",
   "metadata": {},
   "source": [
    "#### [[ ROUGE ]](https://dacon.io/competitions/open/235671/overview/rules)\n",
    "\n",
    "- DACON의 한국어 문서 추출요약 AI 경진대회에서 사용된 ROUGE 스코어 산식 코드를 사용하였다\n",
    "\n",
    "\n",
    "- 해당 코드는 py-rouge 소스코드를 한글에 맞게 수정하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53fb482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import platform\n",
    "import itertools\n",
    "import collections\n",
    "import pkg_resources  # pip install py-rouge\n",
    "from io import open\n",
    "\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Rouge:\n",
    "    DEFAULT_METRICS = {\"rouge-n\"}\n",
    "    DEFAULT_N = 1\n",
    "    STATS = [\"f\", \"p\", \"r\"]\n",
    "    AVAILABLE_METRICS = {\"rouge-n\", \"rouge-l\", \"rouge-w\"}\n",
    "    AVAILABLE_LENGTH_LIMIT_TYPES = {\"words\", \"bytes\"}\n",
    "    REMOVE_CHAR_PATTERN = re.compile(\"[^A-Za-z0-9가-힣]\")\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics=None,\n",
    "        max_n=None,\n",
    "        limit_length=True,\n",
    "        length_limit=1000,\n",
    "        length_limit_type=\"words\",\n",
    "        apply_avg=True,\n",
    "        apply_best=False,\n",
    "        use_tokenizer=True,\n",
    "        alpha=0.5,\n",
    "        weight_factor=1.0,\n",
    "    ):\n",
    "        self.metrics = metrics[:] if metrics is not None else Rouge.DEFAULT_METRICS\n",
    "        for m in self.metrics:\n",
    "            if m not in Rouge.AVAILABLE_METRICS:\n",
    "                raise ValueError(\"Unknown metric '{}'\".format(m))\n",
    "\n",
    "\n",
    "        self.max_n = max_n if \"rouge-n\" in self.metrics else None\n",
    "        # Add all rouge-n metrics\n",
    "        if self.max_n is not None:\n",
    "            index_rouge_n = self.metrics.index(\"rouge-n\")\n",
    "            del self.metrics[index_rouge_n]\n",
    "            self.metrics += [\"rouge-{}\".format(n) for n in range(1, self.max_n + 1)]\n",
    "        self.metrics = set(self.metrics)\n",
    "\n",
    "\n",
    "        self.limit_length = limit_length\n",
    "        if self.limit_length:\n",
    "            if length_limit_type not in Rouge.AVAILABLE_LENGTH_LIMIT_TYPES:\n",
    "                raise ValueError(\"Unknown length_limit_type '{}'\".format(length_limit_type))\n",
    "\n",
    "\n",
    "        self.length_limit = length_limit\n",
    "        if self.length_limit == 0:\n",
    "            self.limit_length = False\n",
    "        self.length_limit_type = length_limit_type\n",
    "\n",
    "\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        if use_tokenizer:\n",
    "            self.tokenizer = Mecab()\n",
    "\n",
    "\n",
    "        self.apply_avg = apply_avg\n",
    "        self.apply_best = apply_best\n",
    "        self.alpha = alpha\n",
    "        self.weight_factor = weight_factor\n",
    "        if self.weight_factor <= 0:\n",
    "            raise ValueError(\"ROUGE-W weight factor must greater than 0.\")\n",
    "\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        if self.use_tokenizer:\n",
    "            return self.tokenizer.morphs(text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def split_into_sentences(text):\n",
    "        return text.split(\"\\n\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_ngrams(n, text):\n",
    "        ngram_set = collections.defaultdict(int)\n",
    "        max_index_ngram_start = len(text) - n\n",
    "        for i in range(max_index_ngram_start + 1):\n",
    "            ngram_set[tuple(text[i : i + n])] += 1\n",
    "        return ngram_set\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_into_words(sentences):\n",
    "        return list(itertools.chain(*[_.split() for _ in sentences]))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_word_ngrams_and_length(n, sentences):\n",
    "        assert len(sentences) > 0\n",
    "        assert n > 0\n",
    "\n",
    "\n",
    "        tokens = Rouge._split_into_words(sentences)\n",
    "        return Rouge._get_ngrams(n, tokens), tokens, len(tokens) - (n - 1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unigrams(sentences):\n",
    "        assert len(sentences) > 0\n",
    "\n",
    "\n",
    "        tokens = Rouge._split_into_words(sentences)\n",
    "        unigram_set = collections.defaultdict(int)\n",
    "        for token in tokens:\n",
    "            unigram_set[token] += 1\n",
    "        return unigram_set, len(tokens)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_p_r_f_score(\n",
    "        evaluated_count,\n",
    "        reference_count,\n",
    "        overlapping_count,\n",
    "        alpha=0.5,\n",
    "        weight_factor=1.0,\n",
    "    ):\n",
    "        precision = 0.0 if evaluated_count == 0 else overlapping_count / float(evaluated_count)\n",
    "        if weight_factor != 1.0:\n",
    "            precision = precision ** (1.0 / weight_factor)\n",
    "        recall = 0.0 if reference_count == 0 else overlapping_count / float(reference_count)\n",
    "        if weight_factor != 1.0:\n",
    "            recall = recall ** (1.0 / weight_factor)\n",
    "        f1_score = Rouge._compute_f_score(precision, recall, alpha)\n",
    "        return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_f_score(precision, recall, alpha=0.5):\n",
    "        return (\n",
    "            0.0\n",
    "            if (recall == 0.0 or precision == 0.0)\n",
    "            else precision * recall / ((1 - alpha) * precision + alpha * recall)\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ngrams(evaluated_sentences, reference_sentences, n):\n",
    "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "\n",
    "        evaluated_ngrams, _, evaluated_count = Rouge._get_word_ngrams_and_length(\n",
    "            n, evaluated_sentences\n",
    "        )\n",
    "        reference_ngrams, _, reference_count = Rouge._get_word_ngrams_and_length(\n",
    "            n, reference_sentences\n",
    "        )\n",
    "\n",
    "\n",
    "        # Gets the overlapping ngrams between evaluated and reference\n",
    "        overlapping_ngrams = set(evaluated_ngrams.keys()).intersection(set(reference_ngrams.keys()))\n",
    "        overlapping_count = 0\n",
    "        for ngram in overlapping_ngrams:\n",
    "            overlapping_count += min(evaluated_ngrams[ngram], reference_ngrams[ngram])\n",
    "\n",
    "\n",
    "        return evaluated_count, reference_count, overlapping_count\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_ngrams_lcs(evaluated_sentences, reference_sentences, weight_factor=1.0):\n",
    "        def _lcs(x, y):\n",
    "            m = len(x)\n",
    "            n = len(y)\n",
    "            vals = collections.defaultdict(int)\n",
    "            dirs = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i - 1] == y[j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j - 1] + 1\n",
    "                        dirs[i, j] = \"|\"\n",
    "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j]\n",
    "                        dirs[i, j] = \"^\"\n",
    "                    else:\n",
    "                        vals[i, j] = vals[i, j - 1]\n",
    "                        dirs[i, j] = \"<\"\n",
    "\n",
    "\n",
    "            return vals, dirs\n",
    "\n",
    "\n",
    "        def _wlcs(x, y, weight_factor):\n",
    "            m = len(x)\n",
    "            n = len(y)\n",
    "            vals = collections.defaultdict(float)\n",
    "            dirs = collections.defaultdict(int)\n",
    "            lengths = collections.defaultdict(int)\n",
    "\n",
    "\n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if x[i - 1] == y[j - 1]:\n",
    "                        length_tmp = lengths[i - 1, j - 1]\n",
    "                        vals[i, j] = (\n",
    "                            vals[i - 1, j - 1]\n",
    "                            + (length_tmp + 1) ** weight_factor\n",
    "                            - length_tmp ** weight_factor\n",
    "                        )\n",
    "                        dirs[i, j] = \"|\"\n",
    "                        lengths[i, j] = length_tmp + 1\n",
    "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
    "                        vals[i, j] = vals[i - 1, j]\n",
    "                        dirs[i, j] = \"^\"\n",
    "                        lengths[i, j] = 0\n",
    "                    else:\n",
    "                        vals[i, j] = vals[i, j - 1]\n",
    "                        dirs[i, j] = \"<\"\n",
    "                        lengths[i, j] = 0\n",
    "\n",
    "\n",
    "            return vals, dirs\n",
    "\n",
    "\n",
    "        def _mark_lcs(mask, dirs, m, n):\n",
    "            while m != 0 and n != 0:\n",
    "                if dirs[m, n] == \"|\":\n",
    "                    m -= 1\n",
    "                    n -= 1\n",
    "                    mask[m] = 1\n",
    "                elif dirs[m, n] == \"^\":\n",
    "                    m -= 1\n",
    "                elif dirs[m, n] == \"<\":\n",
    "                    n -= 1\n",
    "                else:\n",
    "                    raise UnboundLocalError(\"Illegal move\")\n",
    "\n",
    "\n",
    "            return mask\n",
    "\n",
    "\n",
    "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "\n",
    "        evaluated_unigrams_dict, evaluated_count = Rouge._get_unigrams(evaluated_sentences)\n",
    "        reference_unigrams_dict, reference_count = Rouge._get_unigrams(reference_sentences)\n",
    "\n",
    "\n",
    "        # Has to use weight factor for WLCS\n",
    "        use_WLCS = weight_factor != 1.0\n",
    "        if use_WLCS:\n",
    "            evaluated_count = evaluated_count ** weight_factor\n",
    "            reference_count = 0\n",
    "\n",
    "\n",
    "        overlapping_count = 0.0\n",
    "        for reference_sentence in reference_sentences:\n",
    "            reference_sentence_tokens = reference_sentence.split()\n",
    "            if use_WLCS:\n",
    "                reference_count += len(reference_sentence_tokens) ** weight_factor\n",
    "            hit_mask = [0 for _ in range(len(reference_sentence_tokens))]\n",
    "\n",
    "\n",
    "            for evaluated_sentence in evaluated_sentences:\n",
    "                evaluated_sentence_tokens = evaluated_sentence.split()\n",
    "\n",
    "\n",
    "                if use_WLCS:\n",
    "                    _, lcs_dirs = _wlcs(\n",
    "                        reference_sentence_tokens,\n",
    "                        evaluated_sentence_tokens,\n",
    "                        weight_factor,\n",
    "                    )\n",
    "                else:\n",
    "                    _, lcs_dirs = _lcs(reference_sentence_tokens, evaluated_sentence_tokens)\n",
    "                _mark_lcs(\n",
    "                    hit_mask,\n",
    "                    lcs_dirs,\n",
    "                    len(reference_sentence_tokens),\n",
    "                    len(evaluated_sentence_tokens),\n",
    "                )\n",
    "\n",
    "\n",
    "            overlapping_count_length = 0\n",
    "            for ref_token_id, val in enumerate(hit_mask):\n",
    "                if val == 1:\n",
    "                    token = reference_sentence_tokens[ref_token_id]\n",
    "                    if evaluated_unigrams_dict[token] > 0 and reference_unigrams_dict[token] > 0:\n",
    "                        evaluated_unigrams_dict[token] -= 1\n",
    "                        reference_unigrams_dict[ref_token_id] -= 1\n",
    "\n",
    "\n",
    "                        if use_WLCS:\n",
    "                            overlapping_count_length += 1\n",
    "                            if (\n",
    "                                ref_token_id + 1 < len(hit_mask) and hit_mask[ref_token_id + 1] == 0\n",
    "                            ) or ref_token_id + 1 == len(hit_mask):\n",
    "                                overlapping_count += overlapping_count_length ** weight_factor\n",
    "                                overlapping_count_length = 0\n",
    "                        else:\n",
    "                            overlapping_count += 1\n",
    "\n",
    "\n",
    "        if use_WLCS:\n",
    "            reference_count = reference_count ** weight_factor\n",
    "\n",
    "\n",
    "        return evaluated_count, reference_count, overlapping_count\n",
    "\n",
    "\n",
    "    def get_scores(self, hypothesis, references):\n",
    "        if isinstance(hypothesis, str):\n",
    "            hypothesis, references = [hypothesis], [references]\n",
    "\n",
    "\n",
    "        if type(hypothesis) != type(references):\n",
    "            raise ValueError(\"'hyps' and 'refs' are not of the same type\")\n",
    "\n",
    "\n",
    "        if len(hypothesis) != len(references):\n",
    "            raise ValueError(\"'hyps' and 'refs' do not have the same length\")\n",
    "        scores = {}\n",
    "        has_rouge_n_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]) > 0\n",
    "        )\n",
    "        if has_rouge_n_metric:\n",
    "            scores.update(self._get_scores_rouge_n(hypothesis, references))\n",
    "            # scores = {**scores, **self._get_scores_rouge_n(hypothesis, references)}\n",
    "\n",
    "\n",
    "        has_rouge_l_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"l\"]) > 0\n",
    "        )\n",
    "        if has_rouge_l_metric:\n",
    "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, False))\n",
    "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, False)}\n",
    "\n",
    "\n",
    "        has_rouge_w_metric = (\n",
    "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"w\"]) > 0\n",
    "        )\n",
    "        if has_rouge_w_metric:\n",
    "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, True))\n",
    "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, True)}\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _get_scores_rouge_n(self, all_hypothesis, all_references):\n",
    "        metrics = [metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]\n",
    "\n",
    "\n",
    "        if self.apply_avg or self.apply_best:\n",
    "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS} for metric in metrics}\n",
    "        else:\n",
    "            scores = {\n",
    "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
    "                for metric in metrics\n",
    "            }\n",
    "\n",
    "\n",
    "        for sample_id, (hypothesis, references) in enumerate(zip(all_hypothesis, all_references)):\n",
    "            assert isinstance(hypothesis, str)\n",
    "            has_multiple_references = False\n",
    "            if isinstance(references, list):\n",
    "                has_multiple_references = len(references) > 1\n",
    "                if not has_multiple_references:\n",
    "                    references = references[0]\n",
    "\n",
    "\n",
    "            # Prepare hypothesis and reference(s)\n",
    "            hypothesis = self._preprocess_summary_as_a_whole(hypothesis)\n",
    "            references = (\n",
    "                [self._preprocess_summary_as_a_whole(reference) for reference in references]\n",
    "                if has_multiple_references\n",
    "                else [self._preprocess_summary_as_a_whole(references)]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Compute scores\n",
    "            for metric in metrics:\n",
    "                suffix = metric.split(\"-\")[-1]\n",
    "                n = int(suffix)\n",
    "\n",
    "\n",
    "                # Aggregate\n",
    "                if self.apply_avg:\n",
    "                    # average model\n",
    "                    total_hypothesis_ngrams_count = 0\n",
    "                    total_reference_ngrams_count = 0\n",
    "                    total_ngrams_overlapping_count = 0\n",
    "\n",
    "\n",
    "                    for reference in references:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                        total_hypothesis_ngrams_count += hypothesis_count\n",
    "                        total_reference_ngrams_count += reference_count\n",
    "                        total_ngrams_overlapping_count += overlapping_ngrams\n",
    "\n",
    "\n",
    "                    score = Rouge._compute_p_r_f_score(\n",
    "                        total_hypothesis_ngrams_count,\n",
    "                        total_reference_ngrams_count,\n",
    "                        total_ngrams_overlapping_count,\n",
    "                        self.alpha,\n",
    "                    )\n",
    "\n",
    "\n",
    "                    for stat in Rouge.STATS:\n",
    "                        scores[metric][stat] += score[stat]\n",
    "                else:\n",
    "                    # Best model\n",
    "                    if self.apply_best:\n",
    "                        best_current_score = None\n",
    "                        for reference in references:\n",
    "                            (\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                            score = Rouge._compute_p_r_f_score(\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                                self.alpha,\n",
    "                            )\n",
    "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
    "                                best_current_score = score\n",
    "\n",
    "\n",
    "                        for stat in Rouge.STATS:\n",
    "                            scores[metric][stat] += best_current_score[stat]\n",
    "                    # Keep all\n",
    "                    else:\n",
    "                        for reference in references:\n",
    "                            (\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
    "                            score = Rouge._compute_p_r_f_score(\n",
    "                                hypothesis_count,\n",
    "                                reference_count,\n",
    "                                overlapping_ngrams,\n",
    "                                self.alpha,\n",
    "                            )\n",
    "                            for stat in Rouge.STATS:\n",
    "                                scores[metric][sample_id][stat].append(score[stat])\n",
    "\n",
    "\n",
    "        # Compute final score with the average or the the max\n",
    "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
    "            for metric in metrics:\n",
    "                for stat in Rouge.STATS:\n",
    "                    scores[metric][stat] /= len(all_hypothesis)\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _get_scores_rouge_l_or_w(self, all_hypothesis, all_references, use_w=False):\n",
    "        metric = \"rouge-w\" if use_w else \"rouge-l\"\n",
    "        if self.apply_avg or self.apply_best:\n",
    "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS}}\n",
    "        else:\n",
    "            scores = {\n",
    "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
    "            }\n",
    "\n",
    "\n",
    "        for sample_id, (hypothesis_sentences, references_sentences) in enumerate(\n",
    "            zip(all_hypothesis, all_references)\n",
    "        ):\n",
    "            assert isinstance(hypothesis_sentences, str)\n",
    "            has_multiple_references = False\n",
    "            if isinstance(references_sentences, list):\n",
    "                has_multiple_references = len(references_sentences) > 1\n",
    "                if not has_multiple_references:\n",
    "                    references_sentences = references_sentences[0]\n",
    "\n",
    "\n",
    "            # Prepare hypothesis and reference(s)\n",
    "            hypothesis_sentences = self._preprocess_summary_per_sentence(hypothesis_sentences)\n",
    "            references_sentences = (\n",
    "                [\n",
    "                    self._preprocess_summary_per_sentence(reference)\n",
    "                    for reference in references_sentences\n",
    "                ]\n",
    "                if has_multiple_references\n",
    "                else [self._preprocess_summary_per_sentence(references_sentences)]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Compute scores\n",
    "            # Aggregate\n",
    "            if self.apply_avg:\n",
    "                # average model\n",
    "                total_hypothesis_ngrams_count = 0\n",
    "                total_reference_ngrams_count = 0\n",
    "                total_ngrams_overlapping_count = 0\n",
    "\n",
    "\n",
    "                for reference_sentences in references_sentences:\n",
    "                    (\n",
    "                        hypothesis_count,\n",
    "                        reference_count,\n",
    "                        overlapping_ngrams,\n",
    "                    ) = Rouge._compute_ngrams_lcs(\n",
    "                        hypothesis_sentences,\n",
    "                        reference_sentences,\n",
    "                        self.weight_factor if use_w else 1.0,\n",
    "                    )\n",
    "                    total_hypothesis_ngrams_count += hypothesis_count\n",
    "                    total_reference_ngrams_count += reference_count\n",
    "                    total_ngrams_overlapping_count += overlapping_ngrams\n",
    "\n",
    "\n",
    "                score = Rouge._compute_p_r_f_score(\n",
    "                    total_hypothesis_ngrams_count,\n",
    "                    total_reference_ngrams_count,\n",
    "                    total_ngrams_overlapping_count,\n",
    "                    self.alpha,\n",
    "                    self.weight_factor if use_w else 1.0,\n",
    "                )\n",
    "                for stat in Rouge.STATS:\n",
    "                    scores[metric][stat] += score[stat]\n",
    "            else:\n",
    "                # Best model\n",
    "                if self.apply_best:\n",
    "                    best_current_score = None\n",
    "                    best_current_score_wlcs = None\n",
    "                    for reference_sentences in references_sentences:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams_lcs(\n",
    "                            hypothesis_sentences,\n",
    "                            reference_sentences,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "                        score = Rouge._compute_p_r_f_score(\n",
    "                            total_hypothesis_ngrams_count,\n",
    "                            total_reference_ngrams_count,\n",
    "                            total_ngrams_overlapping_count,\n",
    "                            self.alpha,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "\n",
    "\n",
    "                        if use_w:\n",
    "                            reference_count_for_score = reference_count ** (\n",
    "                                1.0 / self.weight_factor\n",
    "                            )\n",
    "                            overlapping_ngrams_for_score = overlapping_ngrams\n",
    "                            score_wlcs = (\n",
    "                                overlapping_ngrams_for_score / reference_count_for_score\n",
    "                            ) ** (1.0 / self.weight_factor)\n",
    "\n",
    "\n",
    "                            if (\n",
    "                                best_current_score_wlcs is None\n",
    "                                or score_wlcs > best_current_score_wlcs\n",
    "                            ):\n",
    "                                best_current_score = score\n",
    "                                best_current_score_wlcs = score_wlcs\n",
    "                        else:\n",
    "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
    "                                best_current_score = score\n",
    "\n",
    "\n",
    "                    for stat in Rouge.STATS:\n",
    "                        scores[metric][stat] += best_current_score[stat]\n",
    "                # Keep all\n",
    "                else:\n",
    "                    for reference_sentences in references_sentences:\n",
    "                        (\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                        ) = Rouge._compute_ngrams_lcs(\n",
    "                            hypothesis_sentences,\n",
    "                            reference_sentences,\n",
    "                            self.weight_factor if use_w else 1.0,\n",
    "                        )\n",
    "                        score = Rouge._compute_p_r_f_score(\n",
    "                            hypothesis_count,\n",
    "                            reference_count,\n",
    "                            overlapping_ngrams,\n",
    "                            self.alpha,\n",
    "                            self.weight_factor,\n",
    "                        )\n",
    "\n",
    "\n",
    "                        for stat in Rouge.STATS:\n",
    "                            scores[metric][sample_id][stat].append(score[stat])\n",
    "\n",
    "\n",
    "        # Compute final score with the average or the the max\n",
    "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
    "            for stat in Rouge.STATS:\n",
    "                scores[metric][stat] /= len(all_hypothesis)\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _preprocess_summary_as_a_whole(self, summary):\n",
    "        sentences = Rouge.split_into_sentences(summary)\n",
    "\n",
    "\n",
    "        # Truncate\n",
    "        if self.limit_length:\n",
    "            # By words\n",
    "            if self.length_limit_type == \"words\":\n",
    "                summary = \" \".join(sentences)\n",
    "                all_tokens = summary.split()  # Counting as in the perls script\n",
    "                summary = \" \".join(all_tokens[: self.length_limit])\n",
    "\n",
    "\n",
    "            # By bytes\n",
    "            elif self.length_limit_type == \"bytes\":\n",
    "                summary = \"\"\n",
    "                current_len = 0\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    sentence_len = len(sentence)\n",
    "\n",
    "\n",
    "                    if current_len + sentence_len < self.length_limit:\n",
    "                        if current_len != 0:\n",
    "                            summary += \" \"\n",
    "                        summary += sentence\n",
    "                        current_len += sentence_len\n",
    "                    else:\n",
    "                        if current_len > 0:\n",
    "                            summary += \" \"\n",
    "                        summary += sentence[: self.length_limit - current_len]\n",
    "                        break\n",
    "        else:\n",
    "            summary = \" \".join(sentences)\n",
    "\n",
    "\n",
    "        summary = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary.lower()).strip()\n",
    "\n",
    "\n",
    "        tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary))\n",
    "        preprocessed_summary = [\" \".join(tokens)]\n",
    "\n",
    "\n",
    "        return preprocessed_summary\n",
    "\n",
    "\n",
    "    def _preprocess_summary_per_sentence(self, summary):\n",
    "        sentences = Rouge.split_into_sentences(summary)\n",
    "\n",
    "\n",
    "        # Truncate\n",
    "        if self.limit_length:\n",
    "            final_sentences = []\n",
    "            current_len = 0\n",
    "            # By words\n",
    "            if self.length_limit_type == \"words\":\n",
    "                for sentence in sentences:\n",
    "                    tokens = sentence.strip().split()\n",
    "                    tokens_len = len(tokens)\n",
    "                    if current_len + tokens_len < self.length_limit:\n",
    "                        sentence = \" \".join(tokens)\n",
    "                        final_sentences.append(sentence)\n",
    "                        current_len += tokens_len\n",
    "                    else:\n",
    "                        sentence = \" \".join(tokens[: self.length_limit - current_len])\n",
    "                        final_sentences.append(sentence)\n",
    "                        break\n",
    "            # By bytes\n",
    "            elif self.length_limit_type == \"bytes\":\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    sentence_len = len(sentence)\n",
    "                    if current_len + sentence_len < self.length_limit:\n",
    "                        final_sentences.append(sentence)\n",
    "                        current_len += sentence_len\n",
    "                    else:\n",
    "                        sentence = sentence[: self.length_limit - current_len]\n",
    "                        final_sentences.append(sentence)\n",
    "                        break\n",
    "            sentences = final_sentences\n",
    "\n",
    "\n",
    "        final_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence.lower()).strip()\n",
    "\n",
    "\n",
    "            tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence))\n",
    "\n",
    "\n",
    "            sentence = \" \".join(tokens)\n",
    "\n",
    "\n",
    "            final_sentences.append(sentence)\n",
    "\n",
    "\n",
    "        return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae8fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RougeScorer:\n",
    "    def __init__(self):\n",
    "        self.rouge_evaluator = Rouge(\n",
    "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
    "            max_n=2,\n",
    "            limit_length=True,\n",
    "            length_limit=1000,\n",
    "            length_limit_type=\"words\",\n",
    "            use_tokenizer=True,\n",
    "            apply_avg=True,\n",
    "            apply_best=False,\n",
    "            alpha=0.5,  # Default F1_score\n",
    "            weight_factor=1.2,\n",
    "        )\n",
    "\n",
    "    def compute_rouge(self, ref_df, hyp_df):\n",
    "        #ref_df = pd.read_csv(ref_path)\n",
    "        #hyp_df = pd.read_csv(hyp_path)\n",
    "        hyp_df.iloc[:,1] = hyp_df.iloc[:,1].fillna(' ')\n",
    "        ids = ref_df['id']\n",
    "        hyp_df = hyp_df[hyp_df['id'].isin(ids)]\n",
    "        hyp_df.index = ref_df.index\n",
    "\n",
    "        ref_df = ref_df.sort_values(by=[\"id\"])\n",
    "        hyp_df = hyp_df.sort_values(by=[\"id\"])\n",
    "        ref_df[\"id\"] = ref_df[\"id\"].astype(int)\n",
    "        hyp_df[\"id\"] = hyp_df[\"id\"].astype(int)\n",
    "\n",
    "        hyps = [tuple(row) for row in hyp_df.values]\n",
    "        refs = [tuple(row) for row in ref_df.values]\n",
    "\n",
    "        reference_summaries = []\n",
    "        generated_summaries = []\n",
    "\n",
    "        for ref_tp, hyp_tp in zip(refs, hyps):\n",
    "            ref, ref_id = ref_tp\n",
    "            hyp, hyp_id = hyp_tp\n",
    "\n",
    "            assert ref_id == hyp_id\n",
    "\n",
    "            reference_summaries.append(ref)\n",
    "            generated_summaries.append(hyp)\n",
    "\n",
    "        scores = self.rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n",
    "        str_scores = self.format_rouge_scores(scores)\n",
    "        #self.save_rouge_scores(str_scores)\n",
    "        return str_scores\n",
    "\n",
    "    def save_rouge_scores(self, str_scores):\n",
    "        with open(\"rouge_scores.txt\", \"w\") as output:\n",
    "            output.write(str_scores)\n",
    "\n",
    "    def format_rouge_scores(self, scores):\n",
    "    \treturn \"{:.3f},{:.3f},{:.3f}\".format(\n",
    "            scores[\"rouge-1\"][\"f\"],\n",
    "            scores[\"rouge-2\"][\"f\"],\n",
    "            scores[\"rouge-l\"][\"f\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb3dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE\n",
    "rouge = RougeScorer()\n",
    "\n",
    "rouge_target = pd.DataFrame(test_outputs, columns=['summary'])\n",
    "rouge_target['id'] = rouge_target.index\n",
    "rouge_generated = pd.DataFrame(generated_outputs, columns=['summary'])\n",
    "rouge_generated['id'] = rouge_generated.index\n",
    "\n",
    "rouge_result = rouge.compute_rouge(rouge_target,rouge_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bc3238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Rouge ]\n",
      "Rouge-1, Rouge-2, Rouge-L :  0.295,0.066,0.166\n"
     ]
    }
   ],
   "source": [
    "print(\"[ Rouge ]\")\n",
    "print(\"Rouge-1, Rouge-2, Rouge-L : \", rouge_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e87a60",
   "metadata": {},
   "source": [
    "### 인퍼런스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adb93837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original :  하얀 케이크를 자르고 있는 여자가 있다.\n",
      "generated :  </s> 들고 있는 여자가 있다. 하얀을살을 집을 이렇게 자르고 있는 여자가 많이 얼굴이 얼굴을 누 살고 있는 여자가이다. 그래서 너를 천천히 이 노래를 잘르고 있는 내가 있다. 아주 들어가는 앉아서 집에서 촬영을 금액을 그것을 들어르고 있는 여자의 있다. 없었다. 그러니까 그렇게 자르고 좋은 경제가 나르고 있는 대표가 있다. 된다. 밥을 법을 공부를 자르고는 여자가 있다. 요즘은 또, 이것을 사업을 넣고 있는 여자가 조금 우리를 사람들이 사는 하얀 결혼을 집을 집을 나서 거르고 있는 선수가 있다. 가지고 있는 시간이 있다. 이제는 속으로 들들이 있는 여자가 있는데, 지금 옆에 있는 여자가 있습니다.</s>\n"
     ]
    }
   ],
   "source": [
    "text = '하얀 케이크를 자르고 있는 여자가 있다.'\n",
    "input_ids = test_tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = test_model.generate(input_ids,\n",
    "                         do_sample=True,\n",
    "                         max_length=256,\n",
    "                         min_length=16,\n",
    "                         repetition_penalty=1.5,\n",
    "                         no_repeat_ngram_size=3,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.92)\n",
    "generated = test_tokenizer.decode(gen_ids[0])\n",
    "print('original : ', text)\n",
    "print('generated : ', generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "181.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
